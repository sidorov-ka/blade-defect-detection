# Training configuration
training:
  num_epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.00001

# Optimizer configuration
optimizer:
  name: adam
  lr: ${training.learning_rate}
  weight_decay: ${training.weight_decay}

